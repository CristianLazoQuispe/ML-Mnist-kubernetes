# âš¡ FastVisionAPI: Scalable Image Inference with Kubernetes + FastAPI + ONNX

![Made with FastAPI](https://img.shields.io/badge/Powered%20By-FastAPI-00b300?logo=fastapi&logoColor=white)
![Kubernetes](https://img.shields.io/badge/K8s-AutoScaling-blue?logo=kubernetes)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![GitHub Repo](https://img.shields.io/badge/Repo-GitHub-black?logo=github)](https://github.com/CristianLazoQuispe/ML-Mnist-kubernetes)


<img src="assets/test_images/mnist_03.png" width="120" align="right"/>

A blazing fast image classification API built with FastAPI + ONNX for high-performance inference, deployed in Kubernetes with auto-scaling, unit tests, and a hexagonal architecture.

---

## ğŸš€ Features

- ğŸ§  ONNX Runtime (no heavy PyTorch runtime)
- âš¡ FastAPI with automatic `/docs` and `/predict` endpoint
- ğŸŒ Exposed via **Ingress** with custom domain (`mnist.local`)
- ğŸ“¦ Dockerized, ready for Kubernetes
- ğŸ” Auto-scalable via **HPA** + metrics-server
- ğŸ§ª Unit tested with Pytest, CI-ready
- ğŸ§± Hexagonal architecture (Adapters, Core, Domain, API)
- ğŸ“¸ Accepts `multipart/form-data` image uploads

---

## ğŸ§¬ Architecture

```
Client â†’ Ingress â†’ FastAPI â†’ ONNX Inference (inside container)
â†‘                          â†“
Test Job â† Unit Tests â† Hexagonal Core (adapter, domain)
```

---

## ğŸ—‚ï¸ Project Structure

```

.
â”œâ”€â”€ app/                # Main FastAPI + adapter layers
â”œâ”€â”€ core/               # Domain and service logic (hexagonal)
â”œâ”€â”€ assets/             # ONNX model and test images
â”œâ”€â”€ docker/             # Dockerfiles for API and tester
â”œâ”€â”€ k8s/                # Kubernetes YAMLs: deployment, service, ingress, jobs
â”œâ”€â”€ tests/              # Pytest-based unit tests
â”œâ”€â”€ Makefile            # Fast deployment, test and load generation
â””â”€â”€ README.md

```

---

## ğŸ”§ Setup (Local Kubernetes)

```bash
git clone https://github.com/tu_usuario/ml-mnist-kubernetes
cd ml-mnist-kubernetes
make build clean-images push deploy
make test        # Run unit test in-cluster
make stress-test-thread # Run stress test 
````

---

## ğŸŒ Try It (After Tunnel)

```bash
# 1. Start tunnel
minikube tunnel

# 2. Visit API
http://mnist.local/docs
```

---

## ğŸ§ª Run Load Test

```bash
make load         # 2000 requests
make load-thread  # 2000 parallel requests
kubectl top pods
kubectl get hpa
```

---

## ğŸ§  Predict Programmatically

```python
import requests
files = {"file": open("assets/test_images/mnist_03.png", "rb")}
r = requests.post("http://mnist.local:8000/predict", files=files)
print(r.json())
```

---

## ğŸ”§ Setup (Local Docker Compose)

```bash
make build-compose run-compose logs-compose-tester
make test-compose        # Run unit test in-docker
make stress-test-compose-thread # Run stress in parallel
````

---

## ğŸ§¹ Clean

```bash
make clean
```

---

## ğŸ“„ License

MIT Â© Cristian â€” 2025

---

## ğŸ™Œ Contributing

Pull requests are welcome. If you find value in this repo, feel free to â­ it or share it!

---

## âœ¨ Screenshot (Swagger UI)

<!-- Replace with your screenshot -->

![Swagger UI](assets/fastapi-docs.gif)

---

## ğŸš€ Load Test Comparison

| Metric               | Docker Compose (1 Replica) | Kubernetes (3 Replicas) |
| -------------------- | -------------------------- | ----------------------- |
| **Total Requests**   | 2000                       | 2000                    |
| **Successes**        | 2000                       | 2000                    |
| **Failures**         | 0                          | 0                       |
| **Total Time (s)**   | \~42.96                    | **\~26.87**             |
| **Mean Latency (s)** | \~0.2122                   | **\~0.1244**            |
| **Std Dev (s)**      | \~0.0772                   | **\~0.0552**            |
| **Min Latency (s)**  | \~0.0775                   | **\~0.0377**            |
| **Max Latency (s)**  | \~0.5974                   | **\~0.5646**            |

> ğŸ“Œ *Inference tested using 2000 requests in local environment using 10 threads.*

---

### âœ… Observations

* **Kubernetes with 3 replicas** handles concurrent requests significantly faster than single-container Docker Compose.
* Latency is lower and more stable with Kubernetes due to **horizontal scaling and built-in load balancing**.
* Both deployments achieved **100% success rate**, showing reliability in processing ONNX inference requests.

---
